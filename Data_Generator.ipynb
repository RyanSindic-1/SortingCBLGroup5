{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61dc3eea-7b9a-48ea-9379-e80c7714bac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating datasets with randomized parameters...\n",
      "Dataset 1 created with 1579 parcels and 25 outfeeds\n",
      "Saving dataset 1 to PosiSorterData_O50_1.xlsx...\n",
      "Successfully created 1 datasets with the following paths:\n",
      " - PosiSorterData_O50_1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Code parameters\n",
    "u_parcels = 1500 # Expected number of parcels\n",
    "std_parcels = 400 # Standard dev. of parcels\n",
    "num_datasets = 1 # Number of datasets\n",
    "lower_limit_outfeeds = 15 # Lowest number of outfeeds\n",
    "upper_limit_outfeeds = 16 # Highest number of outfeeds\n",
    "\n",
    "# Function to generate synthetic parcel data\n",
    "def generate_parcel_data(num_parcels=int(np.random.normal(1500, 500)), start_time=\"2025-04-09 09:00:00\"):\n",
    "    base_time = datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    data = []\n",
    "    \n",
    "    # Randomize the number of outfeeds between 20 and 60\n",
    "    total_outfeeds = 25\n",
    "    \n",
    "    for i in range(1, num_parcels + 1):\n",
    "        # Arrival time with incremental randomness\n",
    "        arrival_delta = timedelta(milliseconds=np.random.randint(100, 1500))\n",
    "        base_time += arrival_delta\n",
    "        arrival_time = base_time\n",
    "        \n",
    "        # Parcel dimensions\n",
    "        length = round(np.random.normal(1.2, 0.3), 3)\n",
    "        width = round(np.random.normal(0.7, 0.3), 3)\n",
    "        height = round(np.random.normal(0.5, 0.3), 3)\n",
    "        weight = round(length * width * height * 1000 + np.random.normal(1000, 500), 1)\n",
    "        \n",
    "        # Create outfeed assignments with randomized number of assignments\n",
    "        # Assign to a random number between 10% and 40% of the outfeeds\n",
    "        min_outfeeds = 0\n",
    "        max_outfeeds = max(2, int(total_outfeeds * 0.4))\n",
    "        num_assigned_outfeeds = np.random.randint(min_outfeeds, max_outfeeds + 1)\n",
    "        outfeeds = [False] * total_outfeeds\n",
    "        true_indices = np.random.choice(range(total_outfeeds), size=num_assigned_outfeeds, replace=False)\n",
    "        for idx in true_indices:\n",
    "            outfeeds[idx] = True\n",
    "        \n",
    "        row = [i, arrival_time, length, width, height, weight] + outfeeds\n",
    "        data.append(row)\n",
    "    \n",
    "    columns = ['Parcel Number', 'Arrival Time', 'Length', 'Width', 'Height', 'Weight'] + [f'Outfeed {i}' for i in range(1, total_outfeeds + 1)]\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Function to generate random layout data\n",
    "def generate_layout_data():\n",
    "    layout = {\n",
    "        \"Layout property\": [\n",
    "            \"Belt Speed\",\n",
    "            \"Distance Infeeds to Scanner\",\n",
    "            \"Distance Scanner to Outfeeds\",\n",
    "            \"Distance between Outfeeds\",\n",
    "            \"Distance Infeeds to Arrival\"\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            round(np.random.uniform(1.0, 2.0), 2),\n",
    "            round(np.random.uniform(10.0, 20.0), 1),\n",
    "            round(np.random.uniform(4.0, 6.0), 1),\n",
    "            round(np.random.uniform(1.5, 2.5), 1),\n",
    "            round(np.random.uniform(10.0, 15.0), 1)\n",
    "        ],\n",
    "        \"Unit\": [\"m/s\", \"m\", \"m\", \"m\", \"m\"]\n",
    "    }\n",
    "    return pd.DataFrame(layout)\n",
    "\n",
    "# Generate datasets\n",
    "datasets = []\n",
    "print(\"Generating datasets with randomized parameters...\")\n",
    "for i in range(num_datasets):\n",
    "    # Force at least 1000 parcels\n",
    "    parcels_df = generate_parcel_data(num_parcels=int(np.random.normal(u_parcels, std_parcels)))\n",
    "    \n",
    "    # Ensure we have at least 1000 parcels\n",
    "    while len(parcels_df) < 1000:\n",
    "        parcels_df = generate_parcel_data(num_parcels=int(np.random.normal(u_parcels, std_parcels)))\n",
    "    \n",
    "    print(f\"Dataset {i+1} created with {len(parcels_df)} parcels and {len(parcels_df.columns) - 6} outfeeds\")\n",
    "    layout_df = generate_layout_data()\n",
    "    datasets.append((parcels_df, layout_df))\n",
    "\n",
    "# Save datasets to Excel files\n",
    "output_paths = []\n",
    "for i, (parcels, layout) in enumerate(datasets, start=1):\n",
    "    output_path = f\"PosiSorterData_O50_{i}.xlsx\"  # Fixed path formatting\n",
    "    print(f\"Saving dataset {i} to {output_path}...\")\n",
    "    with pd.ExcelWriter(output_path, datetime_format='YYYY-MM-DD HH:MM:SS.000') as writer:\n",
    "        parcels.to_excel(writer, sheet_name=\"Parcels\", index=False)\n",
    "        layout.to_excel(writer, sheet_name=\"Layout\", index=False)\n",
    "    output_paths.append(output_path)\n",
    "\n",
    "print(f\"Successfully created {len(output_paths)} datasets with the following paths:\")\n",
    "for path in output_paths:\n",
    "    print(f\" - {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ddb0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sklearn.ensemble (from versions: none)\n",
      "ERROR: No matching distribution found for sklearn.ensemble\n"
     ]
    }
   ],
   "source": [
    "pip install joblib sklearn.ensemble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
